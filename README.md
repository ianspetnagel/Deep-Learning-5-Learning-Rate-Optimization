# Deep-Learning-5-Learning-Rate-Optimization

This project deals with Learning Rate Optimization where I use the ADAM, Dropout, and Batch Normalization algorithms.

--> ADAM is very popular and uses indidual parameter learning rates. Combines Momentum and RMS Prop alogorthms.
--> Dropout helps prevent overfitting. Randomly zero out the outputs of neurons in a layer (except the last layer)

**Output:**
*Using ADAM and NOT Batch*

<img width="770" alt="DL_Ass5Out1" src="https://github.com/ianspetnagel/Deep-Learning-3-Linear-Network-Gradient-Descent/assets/62821052/85767986-375a-4c02-8305-117ec47e228d">

*Adding Batch Normalization* - Note the drop in accuracy
<img width="685" alt="DL_Ass5Out2" src="https://github.com/ianspetnagel/Deep-Learning-3-Linear-Network-Gradient-Descent/assets/62821052/31c2b3ff-d300-4eca-9f98-096d9588b44e">

*Running Batch without ADAM* - Accuracy slightly increases
<img width="620" alt="DL_Ass5Out3" src="https://github.com/ianspetnagel/Deep-Learning-3-Linear-Network-Gradient-Descent/assets/62821052/0bd0729b-053a-4a49-af16-1cea0189e7ad">




